\documentclass[ngerman, 12pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{nicefrac}
\usepackage{csquotes}
\usepackage{booktabs}
\usepackage[left=3cm,right=4cm,top=3cm,bottom=3cm]{geometry}
%\usepackage[]{mathpazo,palatino}
\usepackage{fourier}
\usepackage{microtype}
\usepackage{tikz}
\usepackage{pgfplots,siunitx}    
\usepackage[]{mdframed}

\definecolor{mygray}{rgb}{0.9,0.9,0.9}

\newmdenv [linecolor=black,backgroundcolor=mygray,  frametitle=Anmerkung,leftmargin=0.5cm,rightmargin=0.5cm]{infobox}


\usepackage{listings}

\lstset{literate=%
    {Ö}{{\"O}}1
    {Ä}{{\"A}}1
    {Ü}{{\"U}}1
    {ß}{{\ss}}1
    {ü}{{\"u}}1
    {ä}{{\"a}}1
    {ö}{{\"o}}1
    {~}{{\textasciitilde}}1
}

\definecolor{hellgelb}{rgb}{1,1,0.8}
\definecolor{colKeys}{rgb}{0,0,1}
\definecolor{colIdentifier}{rgb}{0,0,0}
\definecolor{colComments}{rgb}{1,0,0}
\definecolor{colString}{rgb}{0,0.5,0}

\makeatletter
\lstdefinestyle{ausgabe}{
  basicstyle=\ttfamily,%
  backgroundcolor=\color{lightgray}%
}
\makeatother
\lstnewenvironment{ausgabe}{\lstset{style=ausgabe}}{} 


\lstset{%
    float=hbp,%
    basicstyle=\ttfamily\small, %
    identifierstyle=\color{colIdentifier}, %
    keywordstyle=\color{colKeys}, %
    stringstyle=\color{colString}, %
    commentstyle=\color{colComments}, %
    columns=flexible, %
    tabsize=2, %
    frame=single, %
    extendedchars=true, %
    showspaces=false, %
    showstringspaces=false, %
    numbers=left, %
    numberstyle=\tiny, %
    breaklines=true, %
    backgroundcolor=\color{hellgelb}, %
    breakautoindent=true, %
    captionpos=b%
}



% body:            Palatino 10pt
% section titles:  Palatino Bold
% formulas:        Euler-VM
%\usepackage{palatino}
%\usepackage{eulervm}
%\linespread{1.08}  % Palatino needs more leading

% body:            CM Roman 11pt
% section titles:  CM Sansserif Demibold Condensed
% formulas:        CM Math
%\usepackage{ccfonts}
%\setkomafont{sectioning}{\fontfamily{cmss}\fontseries{sbc}\selectfont}


\usepackage[ngerman]{babel}
\usepackage{graphicx}
\usepackage{xcolor}

\usepackage{cancel}


% http://www.latex-community.org/forum/viewtopic.php?f=46&t=21038

\usepackage[bookmarksopen=true,pdfsubject={Statistik},pdftitle={Statistik},pdfauthor={Uwe Ziegenhagen},linkcolor=blue,citecolor=blue,urlcolor=blue,colorlinks=true,pdfproducer={PDFLaTeX},pdfcreator={LaTeX 2e},pdftex,backref]{hyperref}

\usepackage{amsmath,amstext}
\usepackage{attachfile}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\def\qs{\text{QS}(a,b)}
\def\sm{\sum\limits_{i=1}^{n}}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\E}{E}

\title{Einführung in die lineare Regression}
\subtitle{-- DRAFT VERSION--}
\author{Uwe Ziegenhagen}

\begin{document}

\maketitle

\subsubsection*{Historie}

\begin{description}
\item[v1.0] 16.03.2009, erste Version hochgeladen
\item[v2.0] 02.03.2013, einen Vorzeichenfehler beseitigt, diverse Gleichungen und Erläuterungen zum besseren Verständnis hinzugefügt.
\item [v3.0] auf Github gewechselt, Metapost gegen TikZ getauscht, einige Erläuterungen verbessert, \LaTeX\ Code aufgeräumt
\item [v3.1] Neues Zahlenbeispiel (noch nicht überall ausgetauscht), Erweiterung um multiple Regression, Quellcodes, Tests, etc. (DRAFT) 
\end{description}

\section{Einführung}

Aus der Wikipedia\footnote{\url{https://de.wikipedia.org/wiki/Lineare_Regression}, Abruf: 24.06.2018}: 

\begin{quote}
\enquote{Die lineare Regression, die einen Spezialfall des allgemeinen Konzepts der Regressionsanalyse darstellt, ist ein statistisches Verfahren, mit dem versucht wird, eine beobachtete abhängige Variable durch eine oder mehrere unabhängige Variablen zu erklären. Das Beiwort \enquote{linear} ergibt sich dadurch, dass die abhängige Variable eine Linearkombination der Regressionskoeffizienten darstellt (aber nicht notwendigerweise der unabhängigen Variablen). Der Begriff Regression bzw. Regression zur Mitte wurde vor allem durch den Statistiker Francis Galton geprägt. 
}\end{quote}

Allgemein wird eine metrische Variable $Y$ betrachtet, die von ein oder mehreren Variablen $X_i$ abhängt. $Y$ nennt man daher auch die \enquote{abhängige Variable} und die $X_i$ die \enquote{unabhängigen Variablen}.  Im eindimensionalen Fall -- wenn es nur eine $X$-Variable gibt -- spricht man von einer einfachen linearen Regression, in höheren Dimensionen von der multiplen Regression.

\section{Einfache lineare Regression}

Im folgenden nutzen wir die Werte aus Tabelle \ref{tab:werte}, um an ihnen die einfache lineare Regression zu erklären. Die Zahlen könnten beispielsweise die verkaufte Menge Cocktails (\(Y\)) in Abhängigkeit vom Verkaufspreis (\(X\)) sein. Die Zahlen wurden so gewählt, dass man gut mit ihnen rechnen kann. 

\begin{center}
\begin{tabular}{cc} \toprule[2pt]
$X$-Wert & $Y$-Wert \\ \midrule
        1 &50\\
        2 &40\\
        3 &45\\
        4 &20\\
        5 &25\\ 
        6 & 15 \\ \bottomrule[2pt]
\end{tabular}
\end{center}
\captionof{table}{Tabelle mit Wertepaaren}\label{tab:werte}\vspace*{1em}

Stellt man die Punkte in einem Streu-Diagramm (auf englisch \enquote{Scatterplot}) wie in Abbildung \ref{fig:scatter} dar, so erkennt man dass mit steigendem Wert von $X$ die Werte von $Y$ sinken.

\begin{center}
\vspace*{1em}\begin{tikzpicture}[xscale=1,yscale=0.1,punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
]

%\draw[help lines,yellow,thin] (0,0) grid (9,60);

\draw [->,thick] (0,0) -- (8,0);
\draw [->,thick] (0,0) -- (0,70);

\node  [punkt] (A) at (1,50){};
\node  [punkt] (B) at (2,40){};
\node  [punkt] (C) at (3,45){};
\node  [punkt] (D) at (4,20){};
\node  [punkt] (E) at (5,25){};
\node  [punkt] (F) at (6,15){};

\node  [below] (X) at (3.5,-4){X};
\node  [left] (Y) at (-1,30){Y};

\node  [below] (x1) at (1,0){1};
\node  [below] (x2) at (2,0){2};
\node  [below] (x3) at (3,0){3};
\node  [below] (x4) at (4,0){4};
\node  [below] (x5) at (5,0){5};
\node  [below] (x6) at (6,0){6};
\node  [below] (x7) at (7,0){7};

\node  [left] (y1) at (0,10){10};
\node  [left] (y2) at (0,20){20};
\node  [left] (y3) at (0,30){30};
\node  [left] (y4) at (0,40){40};
\node  [left] (y5) at (0,50){50};
\node  [left] (y6) at (0,60){60};

%\draw[blue, very thick](0,0.8)--(5,4.6);
%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
%\draw (A) -- (B);
\end{tikzpicture}
\captionof{figure}{Scatterplot zur Darstellung der X-Y Wertepaare}\label{fig:scatter}\vspace*{1em}
\end{center}

Wenn wir den Zusammenhang dieser Punkte mittels Gerade (also \enquote{linear}) modellieren wollen, unterstellen wir ein Modell der Form: 
   
\begin{equation}
	 Y_i = b + a \cdot x_i + \epsilon_i
\end{equation}

\begin{center}
\begin{tikzpicture}[scale=1,
    punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
]

\draw[help lines,yellow,thin] (0,0) grid (7,6);
\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);
\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};
\draw[blue, very thick](0,1)--(6,4);
\draw[red, very thick](2,2)--(5,2);
\draw[green, very thick](5,2)--(5,3.5);
\draw[magenta, very thick](-0.1,0.1)--(-0.1,0.9);
\node  [left] (b) at (0,1){b};
\node  [right] (y) at (5,2.75){\(\Delta y\)};
\node  [below] (x) at (3.5,2){\(\Delta x\)};
\end{tikzpicture}
\captionof{figure}{Grafische Erläuterung}\label{fig:grafisch}
\end{center}

\(b\) ist dabei der Achsenabschnitt, also der Punkt \((0,b)\), an dem die Y-Achse geschnitten wird. $a$ hingegen ist der Parameter für die Steigung der Regressionsgeraden, also das Verhältnis von $\Delta y$ und $\Delta x$. $a$ und $b$ sind für unsere fünf Wertepaare zu bestimmen. 

\(\epsilon_i\) steht für die Fehler, den wir bei der Modellierung machen, darauf kommen wir später noch zu sprechen. 

Abbildung \ref{fig:grafisch} beschreibt diesen Zusammenhang grafisch: der Achsenabschnitt ist der Schnittpunkt der Geraden mit der Y-Achse, der Anstieg das Verhältnis von \(\Delta y\) zu \(\Delta x\).

Wir können wir nun die Regressionsgerade durch die Punkte zeichnen? Abbildung~\ref{fig:geraden} zeigt zwei Beispiele für beliebig gewählte Regressionsgeraden. Im linken Plot erkennt man, dass die Gerade schon recht gut zu unseren Punkten passt. Im rechten Plot stimmt die Richtung nicht, die Gerade impliziert nämlich dass mit steigendem $X$ die Werte von $Y$ ebenfalls steigen.

\begin{tikzpicture}[xscale=0.8,yscale=0.1,punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
]

%\draw[help lines,yellow,thin] (0,0) grid (9,60);

\draw [->,thick] (0,0) -- (8,0);
\draw [->,thick] (0,0) -- (0,70);

\node  [punkt] (A) at (1,50){};
\node  [punkt] (B) at (2,40){};
\node  [punkt] (C) at (3,45){};
\node  [punkt] (D) at (4,20){};
\node  [punkt] (E) at (5,25){};
\node  [punkt] (F) at (6,15){};

\node  [below] (X) at (3.5,-4){X};
\node  [left] (Y) at (-1,30){Y};

\node  [below] (x1) at (1,0){1};
\node  [below] (x2) at (2,0){2};
\node  [below] (x3) at (3,0){3};
\node  [below] (x4) at (4,0){4};
\node  [below] (x5) at (5,0){5};
\node  [below] (x6) at (6,0){6};
\node  [below] (x7) at (7,0){7};

\node  [left] (y1) at (0,10){10};
\node  [left] (y2) at (0,20){20};
\node  [left] (y3) at (0,30){30};
\node  [left] (y4) at (0,40){40};
\node  [left] (y5) at (0,50){50};
\node  [left] (y6) at (0,60){60};




\draw[blue, very thick](1,60)--(6,5);
%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
%\draw (A) -- (B);
\end{tikzpicture}\begin{tikzpicture}[xscale=0.8,yscale=0.1,punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
]

%\draw[help lines,yellow,thin] (0,0) grid (9,60);

\draw [->,thick] (0,0) -- (8,0);
\draw [->,thick] (0,0) -- (0,70);

\node  [punkt] (A) at (1,50){};
\node  [punkt] (B) at (2,40){};
\node  [punkt] (C) at (3,45){};
\node  [punkt] (D) at (4,20){};
\node  [punkt] (E) at (5,25){};
\node  [punkt] (F) at (6,15){};

\node  [below] (X) at (3.5,-4){X};
\node  [left] (Y) at (-1,30){Y};

\node  [below] (x1) at (1,0){1};
\node  [below] (x2) at (2,0){2};
\node  [below] (x3) at (3,0){3};
\node  [below] (x4) at (4,0){4};
\node  [below] (x5) at (5,0){5};
\node  [below] (x6) at (6,0){6};
\node  [below] (x7) at (7,0){7};

\node  [left] (y1) at (0,10){10};
\node  [left] (y2) at (0,20){20};
\node  [left] (y3) at (0,30){30};
\node  [left] (y4) at (0,40){40};
\node  [left] (y5) at (0,50){50};
\node  [left] (y6) at (0,60){60};


\draw[blue, very thick](1,10)--(6,60);

\end{tikzpicture}
\captionof{figure}{Zwei Regressionsgeraden}\label{fig:geraden}

Da aber eine Einschätzung wie \enquote{recht gut} nicht wirklich mathematisch exakt ist, müssen wir diesen Punkt ein wenig genauer betrachten.

Betrachten wir dazu Abbildung \ref{fig:geraden2}. Hier wurden auf der blauen Geraden die Punkte in grün markiert, die eine Regressionsgleichung für den jeweiligen Wert von $X$ vorhersagt, außerdem wurden die jeweiligen Abstände zwischen dem wahren $Y$-Wert und dem geschätzten $Y$-Wert (den wir ab jetzt $\hat Y$ nennen) markiert. 

\begin{tikzpicture}[scale=1,
    pred/.style={circle, minimum size = 0.04cm,blue,fill=green},
    punkt/.style={circle, minimum size = 0.04cm,blue,fill=red},
]

%\draw[help lines,yellow,thin] (0,0) grid (7,6);


\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);

\draw[blue, very thick](0,6)--(6,2);


\node  [punkt] (A) at (1,1){};
\node  [punkt] (B) at (2,3){};
\node  [punkt] (C) at (3,2){};
\node  [punkt] (D) at (4,5){};
\node  [punkt] (E) at (5,4){};

\node  [pred] (Ap) at (1,5.3333){};
\node  [pred] (Bp) at (2,4.6666){};
\node  [pred] (Cp) at (3,4){};
\node  [pred] (Dp) at (4,3.3333){};
\node  [pred] (Ep) at (5,2.6666){};

\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};
\node  [below] (suba) at (3.5,0){(a)};

%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
\draw[magenta] (A) -- (Ap);
\draw[magenta] (B) -- (Bp);
\draw[magenta] (C) -- (Cp);
\draw[magenta] (D) -- (Dp);
\draw[magenta] (E) -- (Ep);
\end{tikzpicture}\begin{tikzpicture}[scale=1,
    punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
    pred/.style={circle, minimum size = 0.05cm,blue,fill=green},
]

%\draw[help lines,yellow,thin] (0,0) grid (7,6);

\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);

\draw[blue, very thick](0,1)--(6,5);

\node  [punkt] (A) at (1,1){};
\node  [punkt] (B) at (2,3){};
\node  [punkt] (C) at (3,2){};
\node  [punkt] (D) at (4,5){};
\node  [punkt] (E) at (5,4){};

\node  [pred] (Ap) at (1,1.6666){};
\node  [pred] (Bp) at (2,2.3333){};
\node  [pred] (Cp) at (3,3){};
\node  [pred] (Dp) at (4,3.6666){};
\node  [pred] (Ep) at (5,4.3333){};

\draw[magenta] (A) -- (Ap);
\draw[magenta] (B) -- (Bp);
\draw[magenta] (C) -- (Cp);
\draw[magenta] (D) -- (Dp);
\draw[magenta] (E) -- (Ep);


\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};

\node  [below] (subb) at (3.5,0){(b)};

%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
%\draw (A) -- (B);
\end{tikzpicture}
\captionof{figure}{Zwei Regressionsgeraden}\label{fig:geraden2}

Wenn wir beide Grafiken betrachten, so ist schnell sichtbar, dass die Regressionsgerade im linken Bild deutlich schlechter ist als die Regressionsgerade im rechten Bild: die Summe der Abstände zwischen den wahren, roten, Punkten und den durch die Gerade geschätzten Punkten ist viel größer.

Aus dieser Tatsache lässt sich ein sehr wichtiger Schluss ziehen: wenn wir diese Summe der Abstände minimieren könnten, würden wir die optimale Gerade erhalten. In Gleichungsform können wir schreiben:

\begin{equation}
S = \sm y_i - \hat{y}_i\label{eq:summedifferenzen}
\end{equation}

In Worten: S ist die Summe aller Differenzen von wahrem und geschätzten $y$-Wert.

Es hat sich als mathematisch sinnvoll herausgestellt, nicht einfach die Summe der Abstände zu minimieren, sondern die Summe der \textit{quadrierten} Abstände. Es lässt sich nicht nur leicht damit rechnen, der Kleinste-Quadrate-Schätzer ist auch -- sofern die  Annahmen des klassischen linearen Regressionsmodells nicht verletzt sind -- BLUE (\enquote{Best Linear Unbiased Estimator}). Dazu später mehr\ldots

Aus Gleichung \ref{eq:summedifferenzen} wird jetzt -- da wir ja die Quadratsumme minimieren wollen -- die folgende Gleichung:

\begin{equation}
QS = \left(\sm y_i - \hat{y}_i\right)^2\label{eq:qabstaende}
\end{equation}

Diese Quadratsumme der Abweichungen ist nur abhängig von den Parametern \(a\) und \(b\) der Regressionsgleichung, daher können wir schreiben:

\begin{equation}
QS(a,b) = \left(\sm y_i - \hat{y}_i\right)^2
\end{equation}%\label{eq:qabstaendea}


%Die Vorgehensweise bei der linearen Regression veranschaulicht folgende Grafik. Gegeben sind Wertepaare $x_i,y_i$, als schwarze Punkte eingezeichnet. Grün sind die Werte $(\hat{x},\hat{y})$ die durch die lineare Regressionsfunktion errechnet werden. Die roten Linien symbolisieren die Abweichungen\footnote{Es ist egal, ob man $y_i-\hat{y}_i$ oder $\hat{y}_i-y_i$ schreibt, durch die Quadrierung heben sich eventuelle negative Vorzeichen auf.}  $e_i=y_i-\hat{y}_i$ dieser durch die Gleichung bestimmten Punkte von den wahren Punkten. Aufgabe bei der Bestimmung der Parameter ist es nun, $a$ und $b$ so zu wählen, dass die Summe QS der quadrierten Abweichungen -- also $\sum_{i=1}^n (y_i-\hat{y}_i)^2$ -- minimal wird. 

%\begin{center}
%\includegraphics[width=0.75\textwidth]{linreg_metapost.1}
%\end{center}

Im Folgenden werden wir diese Funktion partiell ableiten, um die Gleichungen für die optimalen \(a\) und \(b\) zu ermitteln. 

\section{Herleitung der Parameter-Gleichungen}

Wir schreiben Gleichung \ref{eq:qabstaende} nochmals auf und ersetzen \(\hat y\) durch die Modellgleichung:

\begin{align}\label{eq:1}
\qs	&  = \sm \left(y_i - \hat{y}_i\right)^2\\
								&= \sm \Big(y_i - [ax_i+b]\Big)^2 \label{eq:2}
\end{align}

Da wir die optimalen Werte für die Minimierung dieser Quadratsumme erhalten wollen, bilden wir die partiellen Ableitungen nach $a$ und $b$. Vorher können wir jedoch Gleichung \ref{eq:1} vereinfachen. Mit Hilfe der 2. Binomischen Formel\footnote{\quad 2. Binomische Formel: \((s-t)^2 = s^2 -2st + t^2\)} lösen wir Gleichung \ref{eq:2} auf:

\begin{equation}
	\qs = \sm \Big( \overbrace{y_i^2}^{s^2} - \overbrace{2y_i (ax_i+b)}^{-2st} + \overbrace{(ax_i +b)^2}^{t^2}\Big)
\end{equation}

Da der Term \((ax_i +b)^2\) der 1. Binomischen Formel\footnote{\quad 1. Binomische Formel: \((s+t)^2 = s^2 +2st + t^2\)} entspricht, lösen wir auch diesen auf und vereinfachen:

\begin{equation}
	\qs = \sm \Big(y_i^2 - 2ax_iy_i - 2by_i + \overbrace{a^2x_i^2}^{s^2} + \overbrace{2abx_i}^{2st} + \overbrace{b^2}^{t^2}\Big)\label{eq:simp}
\end{equation}

Ausgehend von Gleichung \ref{eq:simp} bilden wir jetzt die partiellen Ableitungen nach \(a\) und \(b\). 

\begin{align}
\frac{\partial \qs}{\partial a}	&= \sm (-2x_iy_i + 2ax_i^2 + 2bx_i) \label{eq:parta1}\\
								&= 2 \sm x_i(-y_i + ax_i + b) \label{eq:parta2a} \\
								&= 2 \sm x_i(ax_i + b -y_i ) \label{eq:parta2} 
\end{align}

\begin{align}
\frac{\partial \qs}{\partial b}	&= \sm (-2y_i+2ax_i +2b) \label{eq:partb1}\\
								&= 2 \sm (ax_i + b -y_i) \label{eq:partb2}
\end{align}

Wenn wir Gleichung \ref{eq:partb2} nullsetzen und auflösen, erhalten wir 

\begin{alignat}{3}
2\sm ax_i &+ 2\sm b &&- 2 \sm y_i \quad &&= 0 \\
2\sm ax_i &+ \enskip 2nb &&- 2 \sm y_i \quad &&= 0 \\ 
2nb &= 2 \sm y_i &&- 2\sm ax_i && 
\end{alignat}
%& 2 \sm y_i &&- 2\sm ax_i &&= 2nb

Auflösen nach \(b\) (durch \(2n\) teilen) gibt (zusammen mit der Tatsache, dass das arithmetische Mittel allgemein als \(\bar{x}= \frac{1}{n} \sm x_i \) definiert ist):

\begin{align}
b &= \frac{\sm y_i}{n} - \frac{\sm ax_i}{n} \\
	&= \frac{1}{n} \sm y_i - a \frac{1}{n} \sm x_i \\
	&= \bar{y} - a \bar{x}
\end{align}

Setzen wir nun \(b=\bar{y} - a \bar{x}\) in Gleichung \ref{eq:parta2} ein, erhalten wir

\begin{equation}
	2\sm x_i \big(ax_i + (\bar{y} - a \bar{x}) - y_i\big)=0
	\label{eq:einsetz}
\end{equation}

Durch Ausmultiplizieren und Vereinfachen ergibt sich:

\begin{align}
  0 &= \sm x_i \big(ax_i + (\bar{y} - a \bar{x}) - y_i\big) \\
	&= \sm \big(ax_i^2+x_i(\bar{y}-a\bar{x})-x_iy_i \big) \\
     &= \sm \big(ax_i^2 + x_i\bar{y} - a\bar{x}x_i- x_iy_i \big) \\
     &= \sm \big(ax_i^2 - a\bar{x}x_i + x_i\bar{y} - x_iy_i \big) 
\end{align}

\begin{align}
     &= \sm \big( (ax_i^2 - a\bar{x}x_i) + x_i\bar{y} - x_iy_i \big) \\
     &= \sm \big( ax_i^2 - a\bar{x}x_i \big) + \sm x_i\bar{y} - \sm x_iy_i 
\end{align}

Jetzt addiert man \(\sm x_iy_i\) und subtrahiert \(\sm x_i\bar{y}\), um diese beiden Teile auf die andere Seite der Gleichung zu bekommen.

\begin{equation}
\sm (ax_i^2 -ax_i\bar{x}) = \sm x_iy_i - \sm x_i\bar{y} 
\end{equation}

Da \(a\) konstant ist, können wir es vor die Klammer ziehen.

\begin{equation}
a \sm (x_i^2 -x_i\bar{x}) = \sm x_iy_i - \bar{y} \sm x_i 
\end{equation}

Jetzt teilen wir durch \(\sm (x_i^2 -x_i\bar{x})\)

%\begin{equation}
%a \dfrac{\cancel{\sm (x_i^2 -x_i\bar{x})}}{\cancel{\sm (x_i^2 -x_i\bar{x})}}=  \dfrac{\sm x_iy_i - \bar{y} \sm x_i}{\sm (x_i^2 -x_i\bar{x})}
%\end{equation}


\begin{equation}
a =  \dfrac{\sm x_iy_i - \bar{y} \sm x_i}{\sm (x_i^2 -x_i\bar{x})} 
\end{equation}

Aus der Definition des arithmetischen Mittels $\bar{x} = \dfrac{1}{n}\sum x_i$ folgt $\sm x_i= n\bar{x}$. Einsetzen ergibt 

\begin{equation}
a =  \dfrac{\sm x_iy_i - \bar{y} n \bar{x}}{ \sm \big( x_i^2 -  \sm x_i \bar{x} \big)}
\end{equation}


Jetzt zerlegen wir die Summe unter dem Bruchstrich in Einzelsummen und ziehen \(\bar{x}\) vor das zweite Summenzeichen (Zur Erinnerung: konstanter Term!)

\begin{equation}
a =  \dfrac{\sm x_iy_i - \bar{y} n\bar{x}}{\sm x_i^2 - \bar{x} \sm x_i}
\end{equation}

Über Formeln zu Varianz und Kovarianz\footnote{Verschiebungssatz: \\ $\cov(X,Y) = \E \left((X - \E(X))(Y - \E(Y)\right)) =\E(X,Y)- \E(X)\E(Y)$ \\ $\var(X)=\E\left(\left(X-\E(X)\right)^2\right)=\E(X^2)-\left(\E(X)\right)^2$} erhalten wir

\begin{equation}
a =  \dfrac{\sm x_iy_i - n\bar{x}\bar{y}}{\sm x_i^2 -n\bar{x}^2} = \dfrac{n \left(\frac{1}{n}\sm x_iy_i - \bar{x}\bar{y}\right)}{ n\left( \frac{1}{n}\sm x_i^2 -\bar{x}^2\right)}  = \dfrac{n\cov(x,y)}{n\var(x)} = \dfrac{\cov(x,y)}{\var(x)}
\end{equation}

 
\begin{infobox}
Die Kovarianz ist definiert als der Erwartungswert der Produkte der Abweichungen zweier Zufallsvariablen von ihren jeweiligen Populations-Mittelwerten und ist ein Maß für den linearen Zusammenhang. Eine positive Kovarianz bedeutet, dass sich beide Zufallsvariablen in eine Richtung gemeinsam bewegen (positiv-positiv), eine negative Kovarianz bedeutet, dass sich beide Variablen entgegengesetzt bewegen (positiv-negativ).
\end{infobox}


Damit haben wir die beiden Gleichungen hergeleitet, um die Regressionsgerade zu bestimmen:

\begin{equation}
b=\bar{y} - a \bar{x}
\end{equation}


\begin{equation}
a =  \dfrac{\cov(x,y)}{\var(x)}
\end{equation}

Wie lassen sich die beiden Parameter interpretieren? \(b\), die Steigung, gibt den durchschnittlichen Betrag wider, um den sich \(y\) ändert, wenn \(x\) um eine Einheit verändert wird. \(a\) gibt den Betrag von \(y\) an, wenn \(x\) 0 ist. Je nachdem, welche Größen untersucht werden, kann ein \(x\) von 0 sinnvoll interpretiert werden oder nicht. Für eine Untersuchung des  Körpergewichts in Abhängigkeit von der Körpergröße spielt die Körpergröße \(x=0\) sicherlich keine Rolle\ldots    

\clearpage\section{Beispiel}

\begin{center}
\captionof{table}{Hilfstabelle}
\begin{tabular}{r|cccccc} \toprule
Spalte & 1 & 2 & 3 & 4 & 5 & 6 \\
& $x$	&	$y$	&	$x-\bar{x}$	&	$y-\bar{y}$	&	$(x-\bar{x})(y-\bar{y})$	&	$(x-\bar{x})^2$	\\ \midrule
& 1 &  50 &      -2.5 &      17.5 &                -43.75 &          6.25 \\
& 2 &  40 &      -1.5 &       7.5 &                -11.25 &          2.25 \\
& 3 &  45 &      -0.5 &      12.5 &                 -6.25 &          0.25 \\
& 4 &  20 &       0.5 &     -12.5 &                 -6.25 &          0.25 \\
& 5 &  25 &       1.5 &      -7.5 &                -11.25 &          2.25 \\
& 6 &  15 &       2.5 &     -17.5 &                -43.75 &          6.25 \\
\midrule
$\sum$ & 21 & 195 & & & -122.5 & 17.5 \\ \bottomrule
\end{tabular}
\end{center}

Mit Hilfe der Werte aus der Tabelle lassen sich \(a\) und \(b\) einfach bestimmen. Hinweis: \(\bar x = 21/6 = 3.5\), \(\bar y = 195/6 = 32.5 \)

\[ a = \frac{\frac{\sum_{i=1}^n \left(x_i-\bar x \right)\left(y_i-\bar y \right)}{6}}{ 
\frac{\sum_{i=1}^6 \left( x_i - \bar x \right)^2}{6}} = \frac{\sum_{i=1}^n\left(x_i-\bar x \right)\left(y_i-\bar y \right)}{\sum_{i=1}^n \left(x_i-\bar x \right)^2}  = \frac{-122.5}{17.5} = -7 \]

Hinweis zu dieser Rechnung: Brüche werden dividiert, indem man mit dem Kehrwert multipliziert: \( \frac{\nicefrac{1}{6}}{\nicefrac{1}{6}}   = \nicefrac{1}{6} \cdot \nicefrac{6}{1}  = 1 \). Für die Berechnung von \(a\) braucht man die Anzahl der Beobachtungen also nicht mehr.

\[ b = \bar y - a\cdot \bar x = 32.5
 - (-7) \cdot 3.5 = 57.0
 \]

Unsere Regressionsgleichung lautet also:

\[
y = -7 \cdot x + 57
\]


Mit den gefundenen Werten für unsere beiden Parameter können wir jetzt die Regressionsgerade zeichnen:\vspace*{-11mm}


\begin{center}
\vspace*{1em}\begin{tikzpicture}[xscale=1,yscale=0.1,punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
]

%\draw[help lines,yellow,thin] (0,0) grid (9,60);

\draw [->,thick] (0,0) -- (8,0);
\draw [->,thick] (0,0) -- (0,70);

\node  [punkt] (A) at (1,50){};
\node  [punkt] (B) at (2,40){};
\node  [punkt] (C) at (3,45){};
\node  [punkt] (D) at (4,20){};
\node  [punkt] (E) at (5,25){};
\node  [punkt] (F) at (6,15){};

\node  [below] (X) at (3.5,-4){X};
\node  [left] (Y) at (-1,30){Y};

\node  [below] (x1) at (1,0){1};
\node  [below] (x2) at (2,0){2};
\node  [below] (x3) at (3,0){3};
\node  [below] (x4) at (4,0){4};
\node  [below] (x5) at (5,0){5};
\node  [below] (x6) at (6,0){6};
\node  [below] (x7) at (7,0){7};

\node  [left] (y1) at (0,10){10};
\node  [left] (y2) at (0,20){20};
\node  [left] (y3) at (0,30){30};
\node  [left] (y4) at (0,40){40};
\node  [left] (y5) at (0,50){50};
\node  [left] (y6) at (0,60){60};

\draw[blue, very thick](0.5,53.5)--(7,8);
\end{tikzpicture}
\captionof{figure}{Scatterplot mit Regressionsgerade}\label{fig:final}\vspace*{1em}
\end{center}

\section{Maße für die Güte der Linearen Regression}

In diesem Abschnitt möchte ich erläutern, wie man die Stärke des linearen Zusammenhangs und die Güte des linearen Modells bestimmt. Unsere Regressionsparameter \(a\) und \(b\) sind die optimalen Modellparameter für die von uns genutzten Daten, aber sie erklären nicht, wie gut die Werte der unabhängigen Variablen die Werte unserer abhängigen Variablen erklären.

\subsection{Standardfehler der Schätzung}

Der Standardfehler der Schätzung (auf englisch: standard error of estimate\enquote{SSE}) ist ein Maß dafür, wie stark die tatsächlichen \(Y\)-Werte von den geschätzten Werten (\(\hat{Y}\)) abweichen. Je kleiner der Standardfehler, desto besser ist die Erklärung durch unser Modell. 

Berechnet wird der Standardfehler wie folgt:

\begin{equation}
\text{SEE} = \sqrt{\frac{\sum (y-\hat{y})^2}{N-2}}
\end{equation}

\(N-2\) steht für die Anzahl der Freiheitsgrade, der Zahl der Observationen minus der Zahl der geschätzten Parameter (in unserem Beispiel Anstieg und Achsenabschnitt).

Für unser Beispiel ergibt sich SEE daher als:

\begin{center}
\captionof{table}{Berechnung des Standardfehlers}
\begin{tabular}{r|rrrr} \toprule
\(x_i\) & \(y_i\)  & \(\hat{y_i}\) & \(y_i-\hat{y_i}\) & \((y_i-\hat{y_i})^2\) \\ \midrule
1 & 1 & 1.4 & -0.4 & 0.16 \\
2 & 3 & 2.2 & 0.8 & 0.64 \\
3 & 2 & 3.0 & -1.0  & 1.0	\\
4 & 5 & 3.8 & 1.2 & 1.44\\
5 & 4 & 4.6 &  -0.6 & 0.36\\ \midrule
\(\sum\) & & & & 3.6 \\ \bottomrule
\end{tabular}
\end{center}

\begin{equation}
\text{SEE} = \sqrt{\frac{3.6}{3}} = 1.0954451150103321
\end{equation}



\subsection{Korrelationskoeffizient}

Schauen wir uns zuerst den Korrelationskoeffizienten \(r\) an, auch Bravais-Pearson-Koeffizient  genannt. \(r\) misst die Stärke und Richtung des linearen Zusammenhangs zwischen zwei metrischen Variablen. Wichtig ist hier das Wort \enquote{linearen}: \(r\) kann nicht sinnvoll bei nicht-linearen (wie z.\,B. quadratischen) Zusammenhängen genutzt werden.

Für \(r\) gibt es zwei Formeln:

\begin{equation}
r = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}%
{ \sqrt{\sum_{i=1}^n (x_i - \bar{x})^2}  \cdot \sqrt{\sum_{i=1}^n (y_i - \bar{y})^2}}
\end{equation}

\begin{equation}
r = \frac{\sum_{i=1}^n x_iy_i - n\bar{x}\bar{y}}%
{\sqrt{\sum_{i=1}^n x_i^2 - n\bar{x}^2}\cdot \sqrt{\sum_{i=1}^n y_i^2 - n\bar{y}^2}}
\end{equation}

Je nach den gegegeben Zahlen lässt sich besser mal die eine, mal die andere Gleichung nutzen, vom Ergebnis her ergeben beide das gleiche.

\(r\) nimmt Werte zwischen -1 und 1 an, der Wert wird wie folgt interpretiert:

\begin{description}
\item [\(r \approx 1\)] positive Korrelation, bei steigenden Werten von X steigen auch die Werte von Y. Beispiele: Größe und Gewicht einer Person, Körpergröße und Schuhgröße, Außentemperatur und Eis-Absatz an der Eisdiele
\item [\(r \approx -1\)] negative Korrelation, bei steigenden Werten von X sinken die Werte von Y. Beispiele: Außentemperatur und Absatz von Glühwein
\item [\(r \approx 0\)] kein linearer Zusammenhang. Beispiel: Körpergröße und Postleitzahl. Hinweis: Ein \(r\) nahe 0 bedeutet nicht, dass es keinen Zusammenhang gibt, er ist halt nur nicht linear. 
\end{description}

Wichtig ist in diesem Zusammenhang, das \(r\) keine Aussagen über die Kausalität trifft! Man könnte vielleicht für die Korrelation der Anzahl der verkauften Smartphones in Hongkong und der Anzahl der verkauften Eistüten in Vancouver ein positives \(r\) messen, kausal gibt es aber zwischen beiden Variablen keinen Zusammenhang.

Mittels Signifikanztest kann man prüfen, ob der errechnet Korrelationskoeffizient tatsächlich signifikant ist oder nur ein Ergebnis des Zufalls unserer Stichprobe ist. Als Hypothesen formulieren wir

\begin{description}
\item[\(H_0\)] Die Korrelation in der Population ist 0 (\(\rho =0\)).
\item[\(H_1\)] Die Korrelation in der Population ist ungleich 0 (\(\rho\not=0\)).
\end{description}

Als Teststatistik für diesen zweiseitigen Test erhalten wir 

\begin{equation}
t = \frac{r\sqrt{n-2}}{\sqrt{1-r^2}}
\end{equation}

\(t\) ist unter der Nullhypothese \(t\)-verteilt mit \(n-2\) Freiheitsgraden. Den kritischen Wert entnehmen wir aus der Tabelle der \(t\)-Verteilung\footnote{z.\,B. von \url{https://www.statistik.tu-dortmund.de/fileadmin/user_upload/Lehrstuehle/Oekonometrie/Lehre/WiSoOekoSS16/tabelletV.pdf}} bei beispielsweise 95\% und \(n-2\)
und lehnen die Nullhypothese ab, wenn unsere Teststatistik kleiner als \(-1\) mal oder größer als der kritische Wert ist.


\subsection{Bestimmtheitsmaß}

Quadriert man \(r\), so erhält man das sogenannte Bestimmtheitsmaß \(R^2\), je nach Literatur auch \enquote{Determinationskoeffizient} bezeichnet. \(R^2\) ist ein Anteilswert, der die erklärte Varianz in das Verhältnis zur Gesamtvarianz setzt. Daraus folgt, dass \(R^2\) Werte zwischen 0 und 1 (0\% und 100\%) annehmen kann. 

Die Formel dafür lautet:

\begin{equation}
R^2 = r^2 = \frac{\sum_{i=1}^n (\hat{y_i} - \bar{y})^2} {\sum_{i=1}^n (y_i - \bar{y})^2}
\end{equation}

Rechnen wir im nächsten Schritt mal \(R^2\) für unser Zahlenbeispiel aus:

\begin{center}
\captionof{table}{Hilfstabelle für die Berechnung von \(R^2\)}
\begin{tabular}{r|lllr} \toprule
i & \(y_i\)  & \(\hat{y_i}\) & \((\hat{y_i} - \bar{y})^2 \)&  \((y_i - \bar{y})^2\) \\ \midrule
1 & 1 &  1.4 & 2.56    & 4.0\\
2 & 3 &  2.2 &  0.64 & 0.0\\
3 & 2 &  3.0 & 0.0     & 1.0\\ 
4 & 5 &  3.8 & 0.64    & 4.0\\ 
5 & 4 &  4.6 & 2.56    & 1.0\\ \midrule
\(\sum\) & & & 6.4 & 10.0\\ \bottomrule
\end{tabular}
\end{center}

Damit ergibt sich

\begin{equation}
R^2 = \frac{6.4}{10.0} = 64 \%
\end{equation}


Zu beachten ist, dass \(R^2\) -- wie \(r\) -- nur eine Aussage über den \textit{linearen} Zusammenhang trifft, für Daten mit einem nicht-linearen Zusammenhang ist es nicht geeignet.

\section{Berechnung mit dem Taschenrechner}

\subsection{Schultaschenrechner}

Moderne (Schul-)Taschenrechner haben alle entsprechende Funktionen eingebaut, um anhand von übergebenen Wertepaaren die Parameter $a$ und $b$ schnell zu bestimmen. Im Folgenden zeigen wir anhand eines Casio Schultaschenrechners vom Typ Casio fx-991DE X CLASSWIZ, wie es funktioniert. 

\begin{enumerate}
	\item Über den \enquote{Menu}-Button wechselt man in das Statistik-Menü und wählt Punkt 2 für das Modell \texttt{y = x + bx}
	\item Es erscheint das Tabellenblatt für die Eingabe der $x$- und $y$-Werte, hier gibt man jetzt alle 12 Werte ein. Mit \enquote{=} wird ein Wert bestätigt, mit den Pfeiltasten wechselt man zwischen den Zellen.
	\item Nach der Eingabe aller Werte drückt man die \enquote{OPTN}-Taste und gelangt in ein Menü.
	\item Mit \enquote{4} kommt man in das \enquote{Regression}-Untermenü, hier zeigt der Rechner dann an:
	
	\begin{itemize}
		\item das Regressionsmodell \texttt{y = a + bx}
		\item den Wert von a: \texttt{a=57}
		\item den Wert von b: \texttt{b=-7}
		\item sowie den Korrelationskoeffizienten \(r\): \texttt{r=-0.909123767}
		\end{itemize}
	
	\item Drückt man wiederholt \texttt{OPTN} und dann die \enquote{3}, so gelangt man in das Menü für die Berechnung der Variablen. Hier zeigt der Rechner die verschiedenen Teilergebnisse wie \(\bar{x}\), \(\sum{x}\), \(\sum{x^2}\), etc.
	\end{enumerate}

\section{Code-Beispiele}


\subsection{Python}

\lstinputlisting[language=Python,morekeywords={linregress}]{linreg-01.py}


\begin{ausgabe}
Slope -6.999999999999999
Intercept 57.0
R_value -0.909123767204656
P_value 0.012012484314940097
Std_error 1.6035674514745457
-122.5 17.5
   x   y  x-mean_x  y-mean_y  (x-mean_x)(y-mean_y)  (x-mean_x)^2
0  1  50      -2.5      17.5                -43.75          6.25
1  2  40      -1.5       7.5                -11.25          2.25
2  3  45      -0.5      12.5                 -6.25          0.25
3  4  20       0.5     -12.5                 -6.25          0.25
4  5  25       1.5      -7.5                -11.25          2.25
5  6  15       2.5     -17.5                -43.75          6.25 

Slope:  -7.0 
Intersect:  57.0
\end{ausgabe}

\subsection{R}

\lstinputlisting[language=R,morekeywords={lm}]{linreg-01.R}


\begin{ausgabe}
Call:
lm(formula = y ~ x)

Residuals:
         1          2          3          4          5          6 
 9.974e-15 -3.000e+00  9.000e+00 -9.000e+00  3.000e+00  5.112e-15 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   57.000      6.245   9.127 0.000799 ***
x             -7.000      1.604  -4.365 0.012012 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.708 on 4 degrees of freedom
Multiple R-squared:  0.8265,	Adjusted R-squared:  0.7831 
F-statistic: 19.06 on 1 and 4 DF,  p-value: 0.01201
\end{ausgabe}

\subsection{Microsoft Excel}















\section*{Quelldateien}

Dieses Dokument wurde mit \LaTeX, dem freien Textsatzsystem, erstellt. Die Quelldatei dieses Dokuments ist im PDF enthalten, klicken Sie einfach auf das Symbol. Sofern Ihr PDF-Betrachter Attachments unterstützt, sollten Sie auf die Quelldatei zugreifen können.

\begin{tabular}{rl}
  \LaTeX & \attachfile{LinearRegressionPrimer.tex} \\
  Python & \attachfile{linreg-01.py} \\ 
  R & \attachfile{linreg-01.R} \\ 
  Excel & \attachfile{linreg-01.xlsx} \\ 
\end{tabular}



\end{document}


\begin{align}
2\sm ax_i + 2\sm b - 2 \sm y_i &= 0 \\
2\sm ax_i \enskip +\enskip  2nb - 2 \sm y_i &= 0 \\
2nb &= 2\sm y_i - 2 \sm ax_i 
\end{align}

\partial

\stackrel{!}{=}0 


\begin{equation}
	\qs = \sm \Big( \overbrace{y_i^2}^{s^2} - \overbrace{2y_i (ax_i+b)}^{2st} + \overbrace{(ax_i +b)^2}^{t^2}\Big)^2
\end{equation}

Da der Term \((ax_i +b)^2\) der 1. Binomischen Formel entspricht, l�sen wir auch diesen auf: